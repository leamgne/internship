{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9e1845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bc4d641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   folder code                 categories \n",
      "0         P001                      Normal\n",
      "1         P004                      Normal\n",
      "2         P005                      Normal\n",
      "3         P006                      Normal\n",
      "4         P007                      Normal\n",
      "..         ...                         ...\n",
      "67        P159  Early and advanced cancers\n",
      "68        P161  Early and advanced cancers\n",
      "69        P163  Early and advanced cancers\n",
      "70        P166  Early and advanced cancers\n",
      "71        P167  Early and advanced cancers\n",
      "\n",
      "[72 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_excel_file.xlsx' with the actual file path\n",
    "excel_file_path = 'valid_data.xlsx'\n",
    "files_directory = 'D:\\IARC data\\IARC data - LANr'\n",
    "create_directory = 'D:\\IARC data\\dataset'\n",
    "\n",
    "# Replace 'Sheet1' with the sheet name and 'Column_Name' with the column name you want to read\n",
    "sheet_name = 'Sheet1'\n",
    "column_name = ['folder code', 'categories '] \n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Access the specified column\n",
    "selected_column = df[column_name]\n",
    "\n",
    "# Print the column values\n",
    "print(selected_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb5ac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            folder code\n",
      "categories                             \n",
      "Early and advanced cancers           12\n",
      "High grade                           20\n",
      "Low grade                            14\n",
      "Normal                               26\n"
     ]
    }
   ],
   "source": [
    "left_over = selected_column.groupby('categories ').count()\n",
    "print(left_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0cacf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25] [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59] [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]\n"
     ]
    }
   ],
   "source": [
    "categories = [ 'Normal', 'Low grade ','High grade ','Early and advanced cancers']\n",
    "NC = []\n",
    "LG = []\n",
    "HG = []\n",
    "EC = []\n",
    "\n",
    "for index, row in selected_column.iterrows():\n",
    "    if row['categories '] == categories[0]:\n",
    "        NC.append(index)\n",
    "    elif row['categories '] == categories[1]:\n",
    "        LG.append(index)\n",
    "    elif row['categories '] == categories[2]:\n",
    "        HG.append(index)\n",
    "    elif row['categories '] == categories[3]:\n",
    "        EC.append(index)\n",
    "        \n",
    "\n",
    "print(NC, LG, HG, EC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064ac67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P001', 'P004', 'P005', 'P006', 'P007', 'P009', 'P012', 'P013', 'P016', 'P017', 'P018', 'P019', 'P020', 'P021', 'P022', 'P023', 'P024', 'P025', 'P026', 'P027', 'P028', 'P031', 'P032', 'P033', 'P034', 'P035'] ['P081', 'P082', 'P083', 'P084', 'P085', 'P087', 'P088', 'P089', 'P090', 'P091', 'P092', 'P093', 'P094', 'P095'] ['P099', 'P100', 'P103', 'P104', 'P107', 'P109', 'P110', 'P111', 'P113', 'P114', 'P115', 'P117', 'P119', 'P120', 'P124', 'P125', 'P132', 'P134', 'P137', 'P139'] ['P144', 'P148', 'P150', 'P152', 'P154', 'P157', 'P158', 'P159', 'P161', 'P163', 'P166', 'P167']\n"
     ]
    }
   ],
   "source": [
    "NC_code = []\n",
    "LG_code = []\n",
    "HG_code = []\n",
    "EC_code = []\n",
    "\n",
    "for index, code in selected_column.iterrows():\n",
    "    code = code['folder code']\n",
    "    if index in NC:\n",
    "        NC_code.append(code)\n",
    "    elif index in LG:\n",
    "        LG_code.append(code)\n",
    "    elif index in HG:\n",
    "        HG_code.append(code)\n",
    "    elif index in EC:\n",
    "        EC_code.append(code)\n",
    "        \n",
    "print(NC_code, LG_code, HG_code, EC_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1270a97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\IARC data\\dataset\\Normal\\P001\n",
      "D:\\IARC data\\dataset\\Normal\\P004\n",
      "D:\\IARC data\\dataset\\Normal\\P005\n",
      "D:\\IARC data\\dataset\\Normal\\P006\n",
      "D:\\IARC data\\dataset\\Normal\\P007\n",
      "D:\\IARC data\\dataset\\Normal\\P009\n",
      "D:\\IARC data\\dataset\\Normal\\P012\n",
      "D:\\IARC data\\dataset\\Normal\\P013\n",
      "D:\\IARC data\\dataset\\Normal\\P016\n",
      "D:\\IARC data\\dataset\\Normal\\P017\n",
      "D:\\IARC data\\dataset\\Normal\\P018\n",
      "D:\\IARC data\\dataset\\Normal\\P019\n",
      "D:\\IARC data\\dataset\\Normal\\P020\n",
      "D:\\IARC data\\dataset\\Normal\\P021\n",
      "D:\\IARC data\\dataset\\Normal\\P022\n",
      "D:\\IARC data\\dataset\\Normal\\P023\n",
      "D:\\IARC data\\dataset\\Normal\\P024\n",
      "D:\\IARC data\\dataset\\Normal\\P025\n",
      "D:\\IARC data\\dataset\\Normal\\P026\n",
      "D:\\IARC data\\dataset\\Normal\\P027\n",
      "D:\\IARC data\\dataset\\Normal\\P028\n",
      "D:\\IARC data\\dataset\\Normal\\P031\n",
      "D:\\IARC data\\dataset\\Normal\\P032\n",
      "D:\\IARC data\\dataset\\Normal\\P033\n",
      "D:\\IARC data\\dataset\\Normal\\P034\n",
      "D:\\IARC data\\dataset\\LSIL\\P081\n",
      "D:\\IARC data\\dataset\\LSIL\\P082\n",
      "D:\\IARC data\\dataset\\LSIL\\P083\n",
      "D:\\IARC data\\dataset\\LSIL\\P084\n",
      "D:\\IARC data\\dataset\\LSIL\\P085\n",
      "D:\\IARC data\\dataset\\LSIL\\P087\n",
      "D:\\IARC data\\dataset\\LSIL\\P088\n",
      "D:\\IARC data\\dataset\\LSIL\\P089\n",
      "D:\\IARC data\\dataset\\LSIL\\P090\n",
      "D:\\IARC data\\dataset\\LSIL\\P091\n",
      "D:\\IARC data\\dataset\\LSIL\\P092\n",
      "D:\\IARC data\\dataset\\LSIL\\P093\n",
      "D:\\IARC data\\dataset\\LSIL\\P094\n",
      "D:\\IARC data\\dataset\\LSIL\\P095\n",
      "D:\\IARC data\\dataset\\HSIL\\P099\n",
      "D:\\IARC data\\dataset\\HSIL\\P100\n",
      "D:\\IARC data\\dataset\\HSIL\\P103\n",
      "D:\\IARC data\\dataset\\HSIL\\P104\n",
      "D:\\IARC data\\dataset\\HSIL\\P109\n",
      "D:\\IARC data\\dataset\\HSIL\\P111\n",
      "D:\\IARC data\\dataset\\HSIL\\P113\n",
      "D:\\IARC data\\dataset\\HSIL\\P114\n",
      "D:\\IARC data\\dataset\\HSIL\\P115\n",
      "D:\\IARC data\\dataset\\HSIL\\P117\n",
      "D:\\IARC data\\dataset\\HSIL\\P119\n",
      "D:\\IARC data\\dataset\\HSIL\\P120\n",
      "D:\\IARC data\\dataset\\HSIL\\P124\n",
      "D:\\IARC data\\dataset\\HSIL\\P125\n",
      "D:\\IARC data\\dataset\\HSIL\\P134\n",
      "D:\\IARC data\\dataset\\HSIL\\P137\n",
      "D:\\IARC data\\dataset\\HSIL\\P139\n",
      "D:\\IARC data\\dataset\\Cancer\\P144\n",
      "D:\\IARC data\\dataset\\Cancer\\P148\n",
      "D:\\IARC data\\dataset\\Cancer\\P150\n",
      "D:\\IARC data\\dataset\\Cancer\\P152\n",
      "D:\\IARC data\\dataset\\Cancer\\P157\n",
      "D:\\IARC data\\dataset\\Cancer\\P158\n",
      "D:\\IARC data\\dataset\\Cancer\\P159\n",
      "D:\\IARC data\\dataset\\Cancer\\P161\n",
      "D:\\IARC data\\dataset\\Cancer\\P163\n",
      "D:\\IARC data\\dataset\\Cancer\\P166\n",
      "D:\\IARC data\\dataset\\Cancer\\P167\n"
     ]
    }
   ],
   "source": [
    "def create_dir(source, destination_dir):\n",
    "    if os.path.exists(destination_dir):\n",
    "        shutil.rmtree(destination_dir)\n",
    "        shutil.copytree(source,destination_dir)\n",
    "    else:\n",
    "        shutil.copytree(source,destination_dir)\n",
    "    \n",
    "    \n",
    "\n",
    "dataset_type = []\n",
    "\n",
    "for folder in os.listdir(create_directory):\n",
    "    dataset_type.append(folder)\n",
    "    \n",
    "dataset_type.reverse() \n",
    "\n",
    "for folder in os.listdir(files_directory):\n",
    "    full_folder = folder\n",
    "    folder = folder[:4]\n",
    "    if folder in NC_code:\n",
    "        destination_path = os.path.join(create_directory + '\\{}\\{}'.format(dataset_type[0],folder))\n",
    "        print(destination_path)\n",
    "    elif folder in LG_code:\n",
    "        destination_path = os.path.join(create_directory + '\\{}\\{}'.format(dataset_type[1],folder))\n",
    "        print(destination_path)\n",
    "    elif folder in HG_code:\n",
    "        destination_path = os.path.join(create_directory + '\\{}\\{}'.format(dataset_type[2],folder))\n",
    "        print(destination_path)\n",
    "    elif folder in EC_code:\n",
    "        destination_path = os.path.join(create_directory + '\\{}\\{}'.format(dataset_type[3],folder))\n",
    "        print(destination_path)\n",
    "    \n",
    "    source = os.path.join(files_directory + f'\\{full_folder}')\n",
    "    \n",
    "    create_dir(source,destination_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7529dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_and_preprocess_data(dataset_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    label_folders = os.listdir(dataset_folder)\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for label_folder in label_folders:\n",
    "        label_path = os.path.join(dataset_folder, label_folder)\n",
    "        if os.path.isdir(label_path):\n",
    "            for patient_folder in os.listdir(label_path):\n",
    "                patient_path = os.path.join(label_path, patient_folder)\n",
    "                if os.path.isdir(patient_path):\n",
    "                    saline_path = os.path.join(patient_path, 'saline.jpg')\n",
    "                    lugol_path = os.path.join(patient_path, 'lugol.jpg')\n",
    "                    acetic_acid_path = os.path.join(patient_path, 'acetic_acid.jpg')\n",
    "\n",
    "                    for image_path in [saline_path, lugol_path, acetic_acid_path]:\n",
    "                        img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "                        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                        images.append(img_array)\n",
    "                        labels.append(label_encoder.transform([label_folder])[0])\n",
    "\n",
    "    images = np.array(images) / 255.0  # Normalize pixel values\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# ... (rest of the code remains the same)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    dataset_folder = '/path/to/your/dataset'\n",
    "    images, labels = load_and_preprocess_data(dataset_folder)\n",
    "    train_images, val_images, train_labels, val_labels = split_data(images, labels)\n",
    "\n",
    "    model1 = build_efficientnet_b0()\n",
    "    model2 = build_efficientnet_b0()\n",
    "    model3 = build_efficientnet_b0()\n",
    "\n",
    "    # Train the three models separately on their respective inputs\n",
    "    train_model(model1, train_images[:, :, :, :3], train_labels, val_images[:, :, :, :3], val_labels, epochs=10)\n",
    "    train_model(model2, train_images[:, :, :, 3:6], train_labels, val_images[:, :, :, 3:6], val_labels, epochs=10)\n",
    "    train_model(model3, train_images[:, :, :, 6:], train_labels, val_images[:, :, :, 6:], val_labels, epochs=10)\n",
    "\n",
    "    # Concatenate the three models\n",
    "    ensemble_model = models.Sequential([\n",
    "        layers.Input(shape=(224, 224, 3)),\n",
    "        layers.concatenate([model1.layers[-3](layers.Input(shape=(224, 224, 3))),\n",
    "                            model2.layers[-3](layers.Input(shape=(224, 224, 3))),\n",
    "                            model3.layers[-3](layers.Input(shape=(224, 224, 3)))], axis=-1),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4, activation='softmax')  # Assuming 4 classes (normal, LSIL, HSIL, cancer)\n",
    "    ])\n",
    "\n",
    "    ensemble_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Use the ensemble model for prediction\n",
    "    ensemble_predictions = ensemble_model.predict(val_images)\n",
    "\n",
    "    # You can use ensemble_predictions for evaluation or further analysis\n",
    "    # ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eac700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
